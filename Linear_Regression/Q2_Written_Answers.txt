2. Using Cross Validation (CV)

a) Best Lambdas
- Data Set 1, (100-10): 12
- Data Set 2, (100-100): 16
- Data Set 3, (1000-100): 48
- Data Set 4, 50(1000-10): 8
- Data Set 5, 100(1000-10): 8
- Data Set 6, 150(1000-10): 8

b) How do they compare to the lambdas in 1.b)?

The choice of lambda differs for the first three data sets only
(100-10, 100-100, 1000-10), receiving higher values for the optimal
lambda in each instance (12 vs. 8, 16 vs. 0, and 48 vs. 28, respectively).

When it comes to the last three data sets (the 50, 100, and 150
row subsets of 1000-100) the lambda values are same, that is,
8 is the best lambda value for the last three data sets.

c) What are the draw backs of CV?

Generally, Cross-Validation is more computationally expensive, especially 
when the algorithm being tested is complex. Furthermore, using CV
presents no advantage if you have a very small dataset. 

d) What are the factors affecting the performance of CV?

The size of the data set is important once again. Using CV presents no advantage
when you have a very small dataset; as shown in 1b, the Validation (Q1) and CV(Q2) 
approaches don’t differ in their selection of optimal lambda values for the last three 
data sets due to their small size (the 50, 100, and 150 row subsets of 1000-100).

When it comes to K-fold Cross-Validation (which is what we’re doing) 
the value of k becomes important. Usually, the recommended value of k 
for k-fold cross-validation is k = 5 or k = 10, because according to many 
empirical studies these values generate the best bias-variance balance. 
Picking higher values of k risks tipping the model over into high variance 
of the model due to the overlap —— and therefore, the heightened correlation 
—— between the training sets. This problem becomes evident in the 
Leave-One-Out Cross-Validation (LOOCV CV) where k = n which usually 
suffers from high variance and overfitting. 


